ğŸ¤Ÿ SignLoom - Real-Time Sign Language Interpreter


ğŸš€ Overview
SignLoom is a real-time sign language interpreter designed to bridge communication gaps for the deaf and hard-of-hearing community. Using deep learning, computer vision, and a user-friendly interface, it translates sign language gestures into text and speech instantly.

The system is built with MobileNetV2, MediaPipe, OpenCV, and Streamlit, and features both a real-time sign detection module and an interactive tutor to help users learn sign language.

ğŸ§  Problem Statement
Millions of people face daily communication barriers due to hearing disabilities. Traditional methods like lip-reading or note writing are slow and insufficient. Existing sign translation systems often lack real-time performance, accuracy, or accessibility. SignLoom addresses these issues head-on.

ğŸ¯ Key Features
ğŸ¥ Real-time webcam sign recognition

ğŸ§  Transfer learning using MobileNetV2

ğŸ“¦ Custom-trained model with 3000+ gesture images

ğŸ”„ Live Sign âœ Text âœ Speech

ğŸ§‘â€ğŸ« Sign Language Tutor with:

Alphabet, number, and phrase learning

Interactive quizzes

Visual and auditory feedback

ğŸ“„ Sentence recognition with start/stop gestures

ğŸ§© GIF generation for sentence formation

ğŸ”¤ Text-to-Sign translation using mapping logic

ğŸ‘¨â€ğŸ« For Whom?
Deaf and mute individuals

Friends, family, and educators

Healthcare professionals

Emergency responders

Anyone interested in learning sign language

ğŸ”§ Tech Stack

Category | Technology,

Model & Training | TensorFlow, Keras, MobileNetV2,

Hand Detection | MediaPipe

Real-Time Video Capture | OpenCV

Frontend UI | Streamlit

Audio Output | pyttsx3 (Text to Speech)

Data Processing | NumPy, PIL, Scikit-learn

Gesture Augmentation | ImageDataGenerator


ğŸ—ï¸ System Architecture
1.Capture frames from webcam using OpenCV.

2.Detect hand landmarks via MediaPipe.

3.Preprocess ROI and feed to MobileNetV2 model.

4.Display prediction and confidence.

5.Output translated text and speech.

6.Tutor module for visual learning and quizzes.

ğŸ“ˆ Results
ğŸ“Š Achieved 94% accuracy on validation data

âš¡ Real-time prediction within 1â€“2 seconds

ğŸ§ª Tested across varied environments for robustness

ğŸ§© Supports individual signs and sentence-level input

ğŸ§  Tutor quizzes show enhanced user learning retention

ğŸ“¹ Demo
ğŸ¥ Watch our final working demo here:

ğŸ¥ [Watch Final Demo](videos/final_demo_team_8056.mp4)


ğŸ“Œ Future Scope
ğŸ“± Android/iOS app integration

ğŸŒ Support for BSL, ISL, and other regional signs

ğŸ§‘â€ğŸ¤ Facial expression recognition

ğŸ¤– LSTM-based sentence construction

ğŸ§â€â™‚ï¸ Virtual avatar for text-to-sign conversion

ğŸ“¡ Cloud API for wider integration

ğŸ§© Gamified learning modules


ğŸ‘¥ Team

Rudraraju Srihita 

Kalikrishna Prasanna Y





ğŸŒ Let's build a more inclusive world, one gesture at a time. âœ¨

